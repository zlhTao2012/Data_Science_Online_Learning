{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "in_raw = []\n",
    "with open('in.txt', 'r') as lines:\n",
    "    for line in lines:\n",
    "        in_raw.append(map(float, line.split()))\n",
    "        \n",
    "in_sample = pd.DataFrame(in_raw)\n",
    "#print(in_sample)\n",
    "\n",
    "out_raw = []\n",
    "with open('out.txt', 'r') as lines:\n",
    "    for line in lines:\n",
    "        out_raw.append(map(float, line.split()))\n",
    "        \n",
    "out_sample = pd.DataFrame(out_raw)        \n",
    "#print len(out_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "[[  1.00000000e+00  -7.79470210e-01   8.38221380e-01   6.07573808e-01\n",
      "    7.02615082e-01  -6.53368595e-01   1.61769159e+00   5.87511700e-02]\n",
      " [  1.00000000e+00   1.55634910e-01   8.95377430e-01   2.42222252e-02\n",
      "    8.01700742e-01   1.39351986e-01   7.39742520e-01   1.05101234e+00]\n",
      " [  1.00000000e+00  -5.99077030e-02  -7.17779950e-01   3.58893288e-03\n",
      "    5.15208057e-01   4.30005481e-02   6.57872247e-01   7.77687653e-01]\n",
      " [  1.00000000e+00   2.07596360e-01   7.58933380e-01   4.30962487e-02\n",
      "    5.75979875e-01   1.57551807e-01   5.51337020e-01   9.66529740e-01]\n",
      " [  1.00000000e+00  -1.95983120e-01  -3.75487160e-01   3.84093833e-02\n",
      "    1.40990607e-01   7.35891451e-02   1.79504040e-01   5.71470280e-01]\n",
      " [  1.00000000e+00   5.88489470e-01  -8.42553810e-01   3.46319856e-01\n",
      "    7.09896923e-01  -4.95834045e-01   1.43104328e+00   2.54064340e-01]\n",
      " [  1.00000000e+00   7.19858740e-03  -5.48316500e-01   5.18196606e-05\n",
      "    3.00650984e-01  -3.94710425e-03   5.55515087e-01   5.41117913e-01]\n",
      " [  1.00000000e+00   7.38838520e-01  -6.03393690e-01   5.45882359e-01\n",
      "    3.64083945e-01  -4.45810501e-01   1.34223221e+00   1.35444830e-01]\n",
      " [  1.00000000e+00   7.04648080e-01  -2.04200520e-02   4.96528917e-01\n",
      "    4.16978524e-04  -1.43889504e-02   7.25068132e-01   6.84228028e-01]\n",
      " [  1.00000000e+00   9.69926660e-01   6.41371200e-01   9.40757726e-01\n",
      "    4.11357016e-01   6.22083026e-01   3.28555460e-01   1.61129786e+00]\n",
      " [  1.00000000e+00   4.35430990e-01   7.44772540e-01   1.89600147e-01\n",
      "    5.54686136e-01   3.24297044e-01   3.09341550e-01   1.18020353e+00]\n",
      " [  1.00000000e+00  -8.44258220e-01   7.42354230e-01   7.12771942e-01\n",
      "    5.51089803e-01  -6.26738661e-01   1.58661245e+00   1.01903990e-01]\n",
      " [  1.00000000e+00   5.91424710e-01  -5.46021180e-01   3.49783188e-01\n",
      "    2.98139129e-01  -3.22930418e-01   1.13744589e+00   4.54035300e-02]\n",
      " [  1.00000000e+00  -6.90931240e-02   3.76599950e-02   4.77385978e-03\n",
      "    1.41827522e-03  -2.60204670e-03   1.06753119e-01   3.14331290e-02]\n",
      " [  1.00000000e+00  -9.51548650e-01  -7.33055020e-01   9.05444833e-01\n",
      "    5.37369662e-01   6.97537515e-01   2.18493630e-01   1.68460367e+00]\n",
      " [  1.00000000e+00  -1.29881380e-01   7.56760960e-01   1.68691729e-02\n",
      "    5.72687151e-01  -9.82891578e-02   8.86642340e-01   6.26879580e-01]\n",
      " [  1.00000000e+00  -4.95346470e-01  -5.66279080e-01   2.45368125e-01\n",
      "    3.20671996e-01   2.80504343e-01   7.09326100e-02   1.06162555e+00]\n",
      " [  1.00000000e+00  -9.03994130e-01   5.09221500e-01   8.17205387e-01\n",
      "    2.59306536e-01  -4.60333247e-01   1.41321563e+00   3.94772630e-01]\n",
      " [  1.00000000e+00   2.92351280e-01   1.60890150e-01   8.54692709e-02\n",
      "    2.58856404e-02   4.70364413e-02   1.31461130e-01   4.53241430e-01]\n",
      " [  1.00000000e+00   6.47985520e-01  -7.79337690e-01   4.19885234e-01\n",
      "    6.07367235e-01  -5.04999538e-01   1.42732321e+00   1.31352170e-01]\n",
      " [  1.00000000e+00   3.75955740e-01   7.82030870e-02   1.41342718e-01\n",
      "    6.11572282e-03   2.94008994e-02   2.97752653e-01   4.54158827e-01]\n",
      " [  1.00000000e+00   2.45889930e-01   4.51467390e-03   6.04618577e-02\n",
      "    2.03822804e-05   1.11011285e-03   2.41375256e-01   2.50404604e-01]\n",
      " [  1.00000000e+00  -4.57191550e-01   4.23904610e-01   2.09024113e-01\n",
      "    1.79695118e-01  -1.93805606e-01   8.81096160e-01   3.32869400e-02]\n",
      " [  1.00000000e+00  -4.41278760e-01   7.05718920e-01   1.94726944e-01\n",
      "    4.98039194e-01  -3.11418770e-01   1.14699768e+00   2.64440160e-01]\n",
      " [  1.00000000e+00   5.07446690e-01   7.58725860e-01   2.57502143e-01\n",
      "    5.75664931e-01   3.85012926e-01   2.51279170e-01   1.26617255e+00]]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "in_sample_non_linear_transform = np.column_stack((np.ones(len(in_sample)), in_sample[0], in_sample[1], \\\n",
    "                                in_sample[0] * in_sample[0], in_sample[1] * in_sample[1], \\\n",
    "                                in_sample[0] * in_sample[1], abs(in_sample[0] - in_sample[1]), \\\n",
    "                                abs(in_sample[0] + in_sample[1])))\n",
    "\n",
    "print len(in_sample_non_linear_transform)\n",
    "\n",
    "training_in_sample = in_sample_non_linear_transform[0:25, :]\n",
    "print training_in_sample\n",
    "\n",
    "validation_in_sample = in_sample_non_linear_transform[25:, :]\n",
    "print len(validation_in_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_sample_non_linear_transform = np.column_stack((np.ones(len(out_sample)), out_sample[0], out_sample[1], \\\n",
    "                                out_sample[0] * out_sample[0], out_sample[1] * out_sample[1], \\\n",
    "                                out_sample[0] * out_sample[1], abs(out_sample[0] - out_sample[1]), \\\n",
    "                                abs(out_sample[0] + out_sample[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "0.42\n"
     ]
    }
   ],
   "source": [
    "weights_3k = np.dot(np.dot(np.linalg.pinv(np.dot(training_in_sample[:, 0:4].transpose(), training_in_sample[:, 0:4])), training_in_sample[:, 0:4].transpose()), in_sample[2][0:25]) \n",
    "\n",
    "# Calculate the sign of predictions\n",
    "validation_3k_predictions = np.sign(np.dot(validation_in_sample[:, 0:4], weights_3k))\n",
    "\n",
    "# compare the prediction with output\n",
    "errors = gl.SArray(validation_3k_predictions == in_sample[2][25:])\n",
    "\n",
    "validation_3k_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "print validation_3k_errors\n",
    "\n",
    "# Calculate the sign of predictions\n",
    "testing_3k_predictions = np.sign(np.dot(out_sample_non_linear_transform[:, 0:4], weights_3k))\n",
    "\n",
    "# compare the prediction with output\n",
    "errors = gl.SArray(testing_3k_predictions == out_sample[2])\n",
    "\n",
    "testing_3k_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "print testing_3k_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.416\n"
     ]
    }
   ],
   "source": [
    "weights_4k = np.dot(np.dot(np.linalg.pinv(np.dot(training_in_sample[:, 0:5].transpose(), training_in_sample[:, 0:5])), training_in_sample[:, 0:5].transpose()), in_sample[2][0:25]) \n",
    "\n",
    "# Calculate the sign of predictions\n",
    "validation_4k_predictions = np.sign(np.dot(validation_in_sample[:, 0:5], weights_4k))\n",
    "\n",
    "# compare the prediction with output\n",
    "errors = gl.SArray(validation_4k_predictions == in_sample[2][25:])\n",
    "\n",
    "validation_4k_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "print validation_4k_errors\n",
    "\n",
    "# Calculate the sign of predictions\n",
    "testing_4k_predictions = np.sign(np.dot(out_sample_non_linear_transform[:, 0:5], weights_4k))\n",
    "\n",
    "# compare the prediction with output\n",
    "errors = gl.SArray(testing_4k_predictions == out_sample[2])\n",
    "\n",
    "testing_4k_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "print testing_4k_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.188\n"
     ]
    }
   ],
   "source": [
    "weights_5k = np.dot(np.dot(np.linalg.pinv(np.dot(training_in_sample[:, 0:6].transpose(), training_in_sample[:, 0:6])), training_in_sample[:, 0:6].transpose()), in_sample[2][0:25]) \n",
    "\n",
    "# Calculate the sign of predictions\n",
    "validation_5k_predictions = np.sign(np.dot(validation_in_sample[:, 0:6], weights_5k))\n",
    "\n",
    "# compare the prediction with output\n",
    "errors = gl.SArray(validation_5k_predictions == in_sample[2][25:])\n",
    "\n",
    "validation_5k_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "print validation_5k_errors\n",
    "\n",
    "# Calculate the sign of predictions\n",
    "testing_5k_predictions = np.sign(np.dot(out_sample_non_linear_transform[:, 0:6], weights_5k))\n",
    "\n",
    "# compare the prediction with output\n",
    "errors = gl.SArray(testing_5k_predictions == out_sample[2])\n",
    "\n",
    "testing_5k_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "print testing_5k_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.084\n"
     ]
    }
   ],
   "source": [
    "weights_6k = np.dot(np.dot(np.linalg.pinv(np.dot(training_in_sample[:, 0:7].transpose(), training_in_sample[:, 0:7])), training_in_sample[:, 0:7].transpose()), in_sample[2][0:25]) \n",
    "\n",
    "# Calculate the sign of predictions\n",
    "validation_6k_predictions = np.sign(np.dot(validation_in_sample[:, 0:7], weights_6k))\n",
    "\n",
    "# compare the prediction with output\n",
    "errors = gl.SArray(validation_6k_predictions == in_sample[2][25:])\n",
    "\n",
    "validation_6k_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "print validation_6k_errors\n",
    "\n",
    "# Calculate the sign of predictions\n",
    "testing_6k_predictions = np.sign(np.dot(out_sample_non_linear_transform[:, 0:7], weights_6k))\n",
    "\n",
    "# compare the prediction with output\n",
    "errors = gl.SArray(testing_6k_predictions == out_sample[2])\n",
    "\n",
    "testing_6k_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "print testing_6k_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.072\n"
     ]
    }
   ],
   "source": [
    "weights_7k = np.dot(np.dot(np.linalg.pinv(np.dot(training_in_sample[:, 0:8].transpose(), training_in_sample[:, 0:8])), training_in_sample[:, 0:8].transpose()), in_sample[2][0:25]) \n",
    "\n",
    "# Calculate the sign of predictions\n",
    "validation_7k_predictions = np.sign(np.dot(validation_in_sample[:, 0:8], weights_7k))\n",
    "\n",
    "# compare the prediction with output\n",
    "errors = gl.SArray(validation_7k_predictions == in_sample[2][25:])\n",
    "\n",
    "validation_7k_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "print validation_7k_errors\n",
    "\n",
    "# Calculate the sign of predictions\n",
    "testing_7k_predictions = np.sign(np.dot(out_sample_non_linear_transform[:, 0:8], weights_7k))\n",
    "\n",
    "# compare the prediction with output\n",
    "errors = gl.SArray(testing_7k_predictions == out_sample[2])\n",
    "\n",
    "testing_7k_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "print testing_7k_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_25 = in_sample_non_linear_transform[0:25, :]\n",
    "\n",
    "dataset_10 = in_sample_non_linear_transform[25:, :]\n",
    "\n",
    "dataset_25_results = in_sample[2][0:25]\n",
    "dataset_10_results = in_sample[2][25:]\n",
    "\n",
    "def validation(training_dataset, training_dataset_results, validation_dataset, validation_dataset_results, n):\n",
    "\n",
    "    weights = np.dot(np.dot(np.linalg.pinv(np.dot(training_dataset[:, 0:n].transpose(), training_dataset[:, 0:n])), training_dataset[:, 0:n].transpose()), training_dataset_results) \n",
    "\n",
    "    # Calculate the sign of predictions\n",
    "    validation_predictions = np.sign(np.dot(validation_dataset[:, 0:n], weights))\n",
    "\n",
    "    # compare the prediction with output\n",
    "    errors = gl.SArray(validation_predictions == validation_dataset_results)\n",
    "\n",
    "    validation_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "    #print \"training size = %s, k = %s, validation errors is %s\" %(len(training_dataset), n-1, format(validation_errors, '.5f'))\n",
    "    #print sum(errors)\n",
    "\n",
    "    # Calculate the sign of predictions\n",
    "    testing_predictions = np.sign(np.dot(out_sample_non_linear_transform[:, 0:n], weights))\n",
    "\n",
    "    # compare the prediction with output\n",
    "    errors = gl.SArray(testing_predictions == out_sample[2])\n",
    "\n",
    "    testing_errors = 1 - sum(errors) / float(len(errors))\n",
    "\n",
    "    print \"training size = %s, k = %s, testing errors is %s\" %(len(training_dataset), n-1, format(testing_errors, '.5f'))  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size = 25, k = 3, testing errors is 0.42000\n",
      "training size = 25, k = 4, testing errors is 0.41600\n",
      "training size = 25, k = 5, testing errors is 0.18800\n",
      "training size = 25, k = 6, testing errors is 0.08400\n",
      "training size = 25, k = 7, testing errors is 0.07200\n"
     ]
    }
   ],
   "source": [
    "validation(n = 4, training_dataset = dataset_25, training_dataset_results = dataset_25_results, validation_dataset = dataset_10, validation_dataset_results = dataset_10_results)\n",
    "validation(n = 5, training_dataset = dataset_25, training_dataset_results = dataset_25_results, validation_dataset = dataset_10, validation_dataset_results = dataset_10_results)\n",
    "validation(n = 6, training_dataset = dataset_25, training_dataset_results = dataset_25_results, validation_dataset = dataset_10, validation_dataset_results = dataset_10_results)\n",
    "validation(n = 7, training_dataset = dataset_25, training_dataset_results = dataset_25_results, validation_dataset = dataset_10, validation_dataset_results = dataset_10_results)\n",
    "validation(n = 8, training_dataset = dataset_25, training_dataset_results = dataset_25_results, validation_dataset = dataset_10, validation_dataset_results = dataset_10_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size = 10, k = 3, testing errors is 0.39600\n",
      "training size = 10, k = 4, testing errors is 0.38800\n",
      "training size = 10, k = 5, testing errors is 0.28400\n",
      "training size = 10, k = 6, testing errors is 0.19200\n",
      "training size = 10, k = 7, testing errors is 0.19600\n"
     ]
    }
   ],
   "source": [
    "validation(n = 4, training_dataset = dataset_10, training_dataset_results = dataset_10_results, validation_dataset = dataset_25, validation_dataset_results = dataset_25_results)\n",
    "validation(n = 5, training_dataset = dataset_10, training_dataset_results = dataset_10_results, validation_dataset = dataset_25, validation_dataset_results = dataset_25_results)\n",
    "validation(n = 6, training_dataset = dataset_10, training_dataset_results = dataset_10_results, validation_dataset = dataset_25, validation_dataset_results = dataset_25_results)\n",
    "validation(n = 7, training_dataset = dataset_10, training_dataset_results = dataset_10_results, validation_dataset = dataset_25, validation_dataset_results = dataset_25_results)\n",
    "validation(n = 8, training_dataset = dataset_10, training_dataset_results = dataset_10_results, validation_dataset = dataset_25, validation_dataset_results = dataset_25_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "0.22360679775\n",
      "0.316227766017\n",
      "0.282842712475\n",
      "0.360555127546\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def distance(a, b):\n",
    "    return math.sqrt(a**2+b**2)\n",
    "\n",
    "print distance(0.0, 0.1)\n",
    "print distance(0.1, 0.2)\n",
    "print distance(0.1, 0.3)\n",
    "print distance(0.2, 0.2)\n",
    "print distance(0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.108\n"
     ]
    }
   ],
   "source": [
    "print 0.192-0.084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mini_learning(n):\n",
    "    mini_list = list()\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        e1 = np.random.uniform(0, 1)\n",
    "        #print e1\n",
    "\n",
    "        e2 = np.random.uniform(0, 1)\n",
    "        #print e2\n",
    "        \n",
    "        mini_list.append(min(e1, e2))\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return sum(mini_list)/float(len(mini_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3336378526917208"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_learning(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def leave_one_out(p):\n",
    "    data_1 = np.array(([1,-1,0],[1,1,0])) # validation[1,p,1]\n",
    "    data_2 = np.array(([1,-1,0],[1,p,1])) # validation[1,1,0]\n",
    "    data_3 = np.array(([1,p,1],[1,1,0])) # validation[1,-1,0]\n",
    "    \n",
    "    w_1 = np.dot(np.dot(np.linalg.pinv(np.dot(data_1[:,0:2].transpose(), data_1[:,0:2])), data_1[:,0:2].transpose()), data_1[:,2])\n",
    "    error_1 = (np.dot(w_1,np.array(([1,p]))) - 1.)**2\n",
    "\n",
    "    w_2 = np.dot(np.dot(np.linalg.pinv(np.dot(data_2[:,0:2].transpose(), data_2[:,0:2])), data_2[:,0:2].transpose()), data_2[:,2])\n",
    "    error_2 = (np.dot(w_2,np.array(([1,1]))) - 0.)**2\n",
    "    \n",
    "    w_3 = np.dot(np.dot(np.linalg.pinv(np.dot(data_3[:,0:2].transpose(), data_3[:,0:2])), data_3[:,0:2].transpose()), data_3[:,2])\n",
    "    error_3 = (np.dot(w_3,np.array(([1,-1]))) - 0)**2\n",
    "    \n",
    "    return error_1 + error_2 + error_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.40513010306\n",
      "193.994845224\n",
      "1.5\n",
      "2.96065178799\n"
     ]
    }
   ],
   "source": [
    "print leave_one_out(math.sqrt(math.sqrt(3)+4))\n",
    "print leave_one_out(math.sqrt(math.sqrt(3)-1))\n",
    "print leave_one_out(math.sqrt(9+4*math.sqrt(6)))\n",
    "print leave_one_out(math.sqrt(9-math.sqrt(6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_1 = np.array(([1,-1,0],[1,1,0])) validation[1,p,1] then b = 0\n",
    "b_error_1 = (0-1)**2\n",
    "# data_2 = np.array(([1,-1,0],[1,p,1])) validation[1,1,0] then b= 0.5\n",
    "b_error_2 = (0.5-0)**2\n",
    "#data_3 = np.array(([1,p,1],[1,1,0])) # validation[1,-1,0] then b= 0.5\n",
    "b_error_3 = (0.5-0)**2\n",
    "\n",
    "b_error_1 + b_error_2 + b_error_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLA vs SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cvxopt\n",
    "import random\n",
    "import graphlab as gl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_func(x1, x2, slope, point):\n",
    "    output = slope*x1 - slope*point[0] + point[1] - x2\n",
    "    return output\n",
    "\n",
    "def test(N):\n",
    "    max_run = 1000 # No. of repeat the experiment\n",
    "    N = N # sample data size \n",
    "    run = 0 # track the run number, inital\n",
    "    \n",
    "    # PLA inital setup\n",
    "    correct_target = N # no. of correct target to converge \n",
    "    iter_success_list = list() # track the succssfully converged iteration no. in each run     \n",
    "    prediction_results_pla = list() # track the relationship between f and g to calculate P[f(x) != g(x)] \n",
    "    \n",
    "    # SVM inital setup\n",
    "    sv_number_list = list() # count the number of support vectors get in each run\n",
    "    prediction_results_svm = list() # track the relationship between f and g to calculate P[f(x) != g(x)]\n",
    "    \n",
    "    while run < max_run:\n",
    "\n",
    "        # Generate data \n",
    "        features_1 = np.array(([[1]]*N))\n",
    "        features_2 = np.array((np.random.uniform(-1, 1., size=(N, 2)))) # generating two random, uniformly distributed points in [-1, 1]\n",
    "        features = np.hstack((features_1, features_2))\n",
    "\n",
    "        function_f_points = np.array((np.random.uniform(-1, 1., size=(2, 2))))    \n",
    "    \n",
    "        # Calculate the slope\n",
    "        slope = (function_f_points[1,1] - function_f_points[0,1])/(function_f_points[1,0] - function_f_points[0,0])\n",
    "    \n",
    "        # Calculate the output\n",
    "        output = output_func(features_2[:,0], features_2[:,1], slope, function_f_points[0,:])\n",
    "        output = gl.SArray(np.sign(output))\n",
    "        \n",
    "        # Discard the run if all data points are on the one side of the line\n",
    "        if len(output[output == +1]) == 0 or len(output[output == -1]) == 0:\n",
    "            continue  \n",
    "        \n",
    "        # Show the progress\n",
    "        if run <= 10 or (run <= 100 and run % 10 == 0) or (run <= 1000 and run % 100 == 0) \\\n",
    "        or (run <= 10000 and run % 1000 == 0) or run % 10000 == 0:\n",
    "            print \"Run # %d :\" %run\n",
    "            \n",
    "        # generate a random point\n",
    "        features_3 = np.array((np.random.uniform(-1, 1., size=(1, 2)))) \n",
    "        \n",
    "        # Target function classify the random point\n",
    "        target_f = output_func(features_3[:,0], features_3[:,1], slope, function_f_points[0,:])\n",
    "        target_f = gl.SArray(np.sign(target_f))\n",
    "        \n",
    "        #Simple PLA\n",
    "        \n",
    "        # inital values for each run\n",
    "        iteration = 0 # track the iteration number\n",
    "        weights=[0,0,0] # initial vector weights\n",
    "        errors = [0]*N # inital errors\n",
    "\n",
    "        while sum(errors) < correct_target:\n",
    "\n",
    "            # Calculate the sign of predictions\n",
    "            predictions = gl.SArray(np.sign(np.dot(features, weights)))\n",
    "            \n",
    "            # compare the prediction with output\n",
    "            errors = gl.SArray(predictions == output)       \n",
    "\n",
    "            misclassified_points = list() # track the misclassifed points index\n",
    "            mis_points = list() # track the misclassfied points\n",
    "\n",
    "            for num in xrange(N):\n",
    "                if errors[num] == 0:\n",
    "                    misclassified_points.append(num)\n",
    "                    mis_points.append(features[num,:])\n",
    "\n",
    "            if sum(errors) < correct_target:\n",
    "\n",
    "                # Randomly choose one point from misclassified points (choose index in here) \n",
    "                misclassified_point = random.choice(misclassified_points)\n",
    "\n",
    "                # Recalculate the weights\n",
    "                weights += features[misclassified_point, :] * output[misclassified_point]\n",
    "\n",
    "            else:\n",
    "\n",
    "                iter_success_list.append(iteration) # include the successfully converged iteration number\n",
    "\n",
    "                # predicted function classify the random point\n",
    "                predict_pla = weights[0] + weights[1]*features_3[:,0] + weights[2]*features_3[:,1]              \n",
    "                predict_pla = gl.SArray(np.sign(predict_pla))\n",
    "                \n",
    "                \n",
    "                # include the compared classification result of the random point\n",
    "                prediction_results_pla.append(int((list(predict_pla) == list(target_f)) == True))\n",
    "\n",
    "                #print \"PLA Run #\" + str(run) + \": \" + \"successfully converged @ iteration: \" + str(iteration)\n",
    "                #print \"PLA Weights: \" + str(weights)\n",
    "                #print \"PLA Target/Predicted function agree: \" + str(list(predict_pla) == list(target_f))\n",
    "                #print \"======================================================================\"\n",
    "\n",
    "            iteration += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "        # SVM\n",
    "        # Quadratic coefficients\n",
    "        for i in xrange(N):\n",
    "    \n",
    "            f = np.array(([np.dot(output[i]*output[j]*features_2[i,:].transpose(), features_2[j,:]) for j in xrange(N)]))\n",
    "\n",
    "            if i == 0:\n",
    "                qe = np.array(f)\n",
    "            else: \n",
    "                qe = np.vstack((qe, f))\n",
    "        \n",
    "        # cvxopt parameters\n",
    "        P = cvxopt.matrix(qe, tc='d')\n",
    "        q = cvxopt.matrix(np.ones(N) * -1, tc='d')\n",
    "        G = cvxopt.matrix(np.diag(np.ones(N) * -1))\n",
    "        h = cvxopt.matrix(np.zeros(N))\n",
    "        A = cvxopt.matrix(output, (1,N))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        \n",
    "        # Solve the quadratic program\n",
    "        cvxopt.solvers.options['show_progress'] = False # Turn off the 'show_progress'\n",
    "        \n",
    "        sol = cvxopt.solvers.qp(P,q,G,h,A,b)        \n",
    "                \n",
    "        # Lagrange multipliers\n",
    "        all_a = np.ravel(sol['x'])\n",
    "\n",
    "        # Support vectors have non zero lagrange multipliers   \n",
    "        sv_pos = list()\n",
    "        for i in xrange(len(all_a)):\n",
    "            if all_a[i] > 1e-5:\n",
    "                sv_pos.append(i)\n",
    "  \n",
    "        #print \"%d support vectors out of %d points\" % (len(sv_pos), N)                 \n",
    "        sv_number_list.append(len(sv_pos))        \n",
    "        \n",
    "        \n",
    "        a = all_a[sv_pos] # alpha of the support vectors\n",
    "        sv_features = features_2[sv_pos] # features matrix of support vectors\n",
    "        sv_y = np.array(output)[sv_pos]  # output of support vectors\n",
    "            \n",
    "        # Weights    \n",
    "        weights = np.zeros(2)\n",
    "        for n in xrange(len(a)):\n",
    "            weights += a[n] * sv_y[n] * sv_features[n] \n",
    "        \n",
    "        # Intercept\n",
    "        b_list = list()\n",
    "        \n",
    "        for i in sv_pos:  \n",
    "            b_temp = 1/float(output[i]) - np.dot(weights,features_2[i]) \n",
    "            b_list.append(b_temp)\n",
    "            \n",
    "        b = sum(b_list) / float(len(b_list)) \n",
    "        weights = np.hstack((b, weights)) # insert the intercept to the weights\n",
    "        \n",
    "\n",
    "        # predicted function classify the random point\n",
    "        predict_svm = weights[0] + weights[1]*features_3[:,0] + weights[2]*features_3[:,1]              \n",
    "        predict_svm = gl.SArray(np.sign(predict_svm))\n",
    "\n",
    "\n",
    "        # include the compared classification result of the random point\n",
    "        prediction_results_svm.append(int((list(predict_svm) == list(target_f)) == True))\n",
    "\n",
    "        #print \"SVM Weights: \" + str(weights) \n",
    "        #print \"SVM Target/Predicted function agree: \" + str(list(predict_svm) == list(target_f))\n",
    "        #print \"======================================================================\"    \n",
    "        \n",
    "        run += 1 # update the run number\n",
    "            \n",
    "    # Simple PLA Calculate the average iteration        \n",
    "    disagree_prob_pla = 1- sum(prediction_results_pla) / float(len(prediction_results_pla))\n",
    "    print \"PLA Probabily that f and g will disagree on their classfication of a random point: \" + '{:.4f}'.format(round(disagree_prob_pla, 4))\n",
    "    \n",
    "    # Linear SVM Calculate the disagree probability\n",
    "    disagree_prob_svm = 1- sum(prediction_results_svm) / float(len(prediction_results_svm))     \n",
    "    print \"SVM Probabily that f and g will disagree on their classfication of a random point: \" + '{:.4f}'.format(round(disagree_prob_svm, 4))\n",
    "    \n",
    "    print \"Avg count of support vectors: \" +  str(sum(sv_number_list) / float(len(sv_number_list)))\n",
    "\n",
    "\n",
    "    return prediction_results_pla, prediction_results_svm\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run # 0 :\n",
      "Run # 1 :\n",
      "Run # 2 :\n",
      "Run # 3 :\n",
      "Run # 4 :\n",
      "Run # 5 :\n",
      "Run # 6 :\n",
      "Run # 7 :\n",
      "Run # 8 :\n",
      "Run # 9 :\n",
      "Run # 10 :\n",
      "Run # 20 :\n",
      "Run # 30 :\n",
      "Run # 40 :\n",
      "Run # 50 :\n",
      "Run # 60 :\n",
      "Run # 70 :\n",
      "Run # 80 :\n",
      "Run # 90 :\n",
      "Run # 100 :\n",
      "Run # 200 :\n",
      "Run # 300 :\n",
      "Run # 400 :\n",
      "Run # 500 :\n",
      "Run # 600 :\n",
      "Run # 700 :\n",
      "Run # 800 :\n",
      "Run # 900 :\n",
      "PLA Probabily that f and g will disagree on their classfication of a random point: 0.1070\n",
      "SVM Probabily that f and g will disagree on their classfication of a random point: 0.0920\n",
      "Avg count of support vectors: 2.883\n"
     ]
    }
   ],
   "source": [
    "prediction_results_pla_10, prediction_results_svm_10 = test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_10 = [1. if x == -1. else 0 for x in np.array(prediction_results_pla_10) - np.array(prediction_results_svm_10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 983,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_10) / float(len(test_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_results_pla_100, prediction_results_svm_100 = test(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_100 = [1. if x == -1. else 0 for x in np.array(prediction_results_pla_100) - np.array(prediction_results_svm_100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 1004,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_100) / float(len(test_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_func(x1, x2, slope, point):\n",
    "    output = slope*x1 - slope*point[0] + point[1] - x2\n",
    "    return output\n",
    "\n",
    "def test(N):\n",
    "    \n",
    "    # PLA inital setup\n",
    "    correct_target = N # no. of correct target to converge \n",
    "    iter_success_list = list() # track the succssfully converged iteration no. in each run     \n",
    "    \n",
    "    # SVM inital setup\n",
    "    sv_number_list = list() # count the number of support vectors get in each run\n",
    "    \n",
    "\n",
    "    # Generate data \n",
    "    features_1 = np.array(([[1]]*N))\n",
    "    features_2 = np.array((np.random.uniform(-1, 1., size=(N, 2)))) # generating two random, uniformly distributed points in [-1, 1]\n",
    "    features = np.hstack((features_1, features_2))\n",
    "\n",
    "    function_f_points = np.array((np.random.uniform(-1, 1., size=(2, 2))))    \n",
    "\n",
    "    # Calculate the slope\n",
    "    slope = (function_f_points[1,1] - function_f_points[0,1])/(function_f_points[1,0] - function_f_points[0,0])\n",
    "\n",
    "    # Calculate the output\n",
    "    output = output_func(features_2[:,0], features_2[:,1], slope, function_f_points[0,:])\n",
    "    output = gl.SArray(np.sign(output))\n",
    "\n",
    "    # Discard the run if all data points are on the one side of the line\n",
    "    if len(output[output == +1]) == 0 or len(output[output == -1]) == 0:\n",
    "        test(N) \n",
    "\n",
    "    # generate a random point\n",
    "    features_3 = np.array((np.random.uniform(-1, 1., size=(100000, 2)))) \n",
    "\n",
    "    # Target function classify the random point\n",
    "    target_f = output_func(features_3[:,0], features_3[:,1], slope, function_f_points[0,:])\n",
    "    target_f = gl.SArray(np.sign(target_f))\n",
    "\n",
    "    #Simple PLA\n",
    "\n",
    "    # inital values for each run\n",
    "    iteration = 0 # track the iteration number\n",
    "    weights=[0,0,0] # initial vector weights\n",
    "    errors = [0]*N # inital errors\n",
    "\n",
    "    while sum(errors) < correct_target:\n",
    "\n",
    "        # Calculate the sign of predictions\n",
    "        predictions = gl.SArray(np.sign(np.dot(features, weights)))\n",
    "\n",
    "        # compare the prediction with output\n",
    "        errors = gl.SArray(predictions == output)       \n",
    "\n",
    "        misclassified_points = list() # track the misclassifed points index\n",
    "        mis_points = list() # track the misclassfied points\n",
    "\n",
    "        for num in xrange(N):\n",
    "            if errors[num] == 0:\n",
    "                misclassified_points.append(num)\n",
    "                mis_points.append(features[num,:])\n",
    "\n",
    "        if sum(errors) < correct_target:\n",
    "\n",
    "            # Randomly choose one point from misclassified points (choose index in here) \n",
    "            misclassified_point = random.choice(misclassified_points)\n",
    "\n",
    "            # Recalculate the weights\n",
    "            weights += features[misclassified_point, :] * output[misclassified_point]\n",
    "\n",
    "        else:\n",
    "\n",
    "            iter_success_list.append(iteration) # include the successfully converged iteration number\n",
    "\n",
    "            # predicted function classify the random point\n",
    "            predict_pla = weights[0] + weights[1]*features_3[:,0] + weights[2]*features_3[:,1]              \n",
    "            predict_pla = gl.SArray(np.sign(predict_pla))\n",
    "\n",
    "\n",
    "            # include the compared classification result of the random point\n",
    "            prediction_results_pla = (gl.SArray(predict_pla) == gl.SArray(target_f))\n",
    "\n",
    "            #print \"PLA Run #\" + str(run) + \": \" + \"successfully converged @ iteration: \" + str(iteration)\n",
    "            #print \"PLA Weights: \" + str(weights)\n",
    "            #print \"PLA Target/Predicted function agree: \" + str(list(predict_pla) == list(target_f))\n",
    "            #print \"======================================================================\"\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "\n",
    "\n",
    "    # SVM\n",
    "    # Quadratic coefficients\n",
    "    for i in xrange(N):\n",
    "\n",
    "        f = np.array(([np.dot(output[i]*output[j]*features_2[i,:].transpose(), features_2[j,:]) for j in xrange(N)]))\n",
    "\n",
    "        if i == 0:\n",
    "            qe = np.array(f)\n",
    "        else: \n",
    "            qe = np.vstack((qe, f))\n",
    "\n",
    "    # cvxopt parameters\n",
    "    P = cvxopt.matrix(qe, tc='d')\n",
    "    q = cvxopt.matrix(np.ones(N) * -1, tc='d')\n",
    "    G = cvxopt.matrix(np.diag(np.ones(N) * -1))\n",
    "    h = cvxopt.matrix(np.zeros(N))\n",
    "    A = cvxopt.matrix(output, (1,N))\n",
    "    b = cvxopt.matrix(0.0)\n",
    "\n",
    "    # Solve the quadratic program\n",
    "    cvxopt.solvers.options['show_progress'] = False # Turn off the 'show_progress'\n",
    "\n",
    "    sol = cvxopt.solvers.qp(P,q,G,h,A,b)        \n",
    "\n",
    "    # Lagrange multipliers\n",
    "    all_a = np.ravel(sol['x'])\n",
    "\n",
    "    # Support vectors have non zero lagrange multipliers   \n",
    "    sv_pos = list()\n",
    "    for i in xrange(len(all_a)):\n",
    "        if all_a[i] > 1e-5:\n",
    "            sv_pos.append(i)\n",
    "\n",
    "    #print \"%d support vectors out of %d points\" % (len(sv_pos), N)                 \n",
    "    sv_number_list.append(len(sv_pos))        \n",
    "\n",
    "\n",
    "    a = all_a[sv_pos] # alpha of the support vectors\n",
    "    sv_features = features_2[sv_pos] # features matrix of support vectors\n",
    "    sv_y = np.array(output)[sv_pos]  # output of support vectors\n",
    "\n",
    "    # Weights    \n",
    "    weights = np.zeros(2)\n",
    "    for n in xrange(len(a)):\n",
    "        weights += a[n] * sv_y[n] * sv_features[n] \n",
    "\n",
    "    # Intercept\n",
    "    b_list = list()\n",
    "\n",
    "    for i in sv_pos:  \n",
    "        b_temp = 1/float(output[i]) - np.dot(weights,features_2[i]) \n",
    "        b_list.append(b_temp)\n",
    "\n",
    "    b = sum(b_list) / float(len(b_list)) \n",
    "    weights = np.hstack((b, weights)) # insert the intercept to the weights\n",
    "\n",
    "\n",
    "    # predicted function classify the random point\n",
    "    predict_svm = weights[0] + weights[1]*features_3[:,0] + weights[2]*features_3[:,1]              \n",
    "    predict_svm = gl.SArray(np.sign(predict_svm))\n",
    "\n",
    "\n",
    "    # include the compared classification result of the random point\n",
    "    prediction_results_svm = (gl.SArray(predict_svm) == gl.SArray(target_f))\n",
    "\n",
    "    #print \"SVM Weights: \" + str(weights) \n",
    "    #print \"SVM Target/Predicted function agree: \" + str(list(predict_svm) == list(target_f))\n",
    "    #print \"======================================================================\"    \n",
    "        \n",
    "\n",
    "            \n",
    "    # Simple PLA Calculate the average iteration        \n",
    "    disagree_prob_pla = 1- sum(prediction_results_pla) / float(len(prediction_results_pla))\n",
    "    print \"PLA Probabily that f and g will disagree on their classfication of a random point: \" + '{:.4f}'.format(round(disagree_prob_pla, 4))\n",
    "    \n",
    "    # Linear SVM Calculate the disagree probability\n",
    "    disagree_prob_svm = 1- sum(prediction_results_svm) / float(len(prediction_results_svm))     \n",
    "    print \"SVM Probabily that f and g will disagree on their classfication of a random point: \" + '{:.4f}'.format(round(disagree_prob_svm, 4))\n",
    "    \n",
    "    print \"Avg count of support vectors: \" +  str(sum(sv_number_list) / float(len(sv_number_list)))\n",
    "\n",
    "\n",
    "    return prediction_results_pla, prediction_results_svm\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLA Probabily that f and g will disagree on their classfication of a random point: 0.2894\n",
      "SVM Probabily that f and g will disagree on their classfication of a random point: 0.0916\n",
      "Avg count of support vectors: 3.0\n"
     ]
    }
   ],
   "source": [
    "prediction_results_pla_10, prediction_results_svm_10 = test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71063\n",
      "90837\n"
     ]
    }
   ],
   "source": [
    "print sum(prediction_results_pla_10)\n",
    "print sum(prediction_results_svm_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLA Probabily that f and g will disagree on their classfication of a random point: 0.0089\n",
      "SVM Probabily that f and g will disagree on their classfication of a random point: 0.0034\n",
      "Avg count of support vectors: 3.0\n"
     ]
    }
   ],
   "source": [
    "prediction_results_pla_100, prediction_results_svm_100 = test(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99110\n",
      "99656\n"
     ]
    }
   ],
   "source": [
    "print sum(prediction_results_pla_100)\n",
    "print sum(prediction_results_svm_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_func(x1, x2, slope, point):\n",
    "    output = slope*x1 - slope*point[0] + point[1] - x2\n",
    "    return output\n",
    "\n",
    "def test(N):\n",
    "    max_run = 1000 # No. of repeat the experiment\n",
    "    N = N # sample data size \n",
    "    run = 0 # track the run number, inital\n",
    "    prediction_results = list()\n",
    "    \n",
    "    # PLA inital setup\n",
    "    correct_target = N # no. of correct target to converge \n",
    "    iter_success_list = list() # track the succssfully converged iteration no. in each run     \n",
    "    \n",
    "    # SVM inital setup\n",
    "    sv_number_list = list() # count the number of support vectors get in each run\n",
    "    \n",
    "    while run < max_run:\n",
    "\n",
    "        # Generate data \n",
    "        features_1 = np.array(([[1]]*N))\n",
    "        features_2 = np.array((np.random.uniform(-1, 1., size=(N, 2)))) # generating two random, uniformly distributed points in [-1, 1]\n",
    "        features = np.hstack((features_1, features_2))\n",
    "\n",
    "        function_f_points = np.array((np.random.uniform(-1, 1., size=(2, 2))))    \n",
    "    \n",
    "        # Calculate the slope\n",
    "        slope = (function_f_points[1,1] - function_f_points[0,1])/(function_f_points[1,0] - function_f_points[0,0])\n",
    "    \n",
    "        # Calculate the output\n",
    "        output = output_func(features_2[:,0], features_2[:,1], slope, function_f_points[0,:])\n",
    "        output = gl.SArray(np.sign(output))\n",
    "        \n",
    "        # Discard the run if all data points are on the one side of the line\n",
    "        if len(output[output == +1]) == 0 or len(output[output == -1]) == 0:\n",
    "            continue  \n",
    "        \n",
    "        # Show the progress\n",
    "        if run <= 10 or (run <= 100 and run % 10 == 0) or (run <= 1000 and run % 100 == 0) \\\n",
    "        or (run <= 10000 and run % 1000 == 0) or run % 10000 == 0:\n",
    "            print \"Run # %d :\" %run\n",
    "            \n",
    "        # generate a random point\n",
    "        features_3 = np.array((np.random.uniform(-1, 1., size=(500, 2)))) \n",
    "        \n",
    "        # Target function classify the random point\n",
    "        target_f = output_func(features_3[:,0], features_3[:,1], slope, function_f_points[0,:])\n",
    "        target_f = gl.SArray(np.sign(target_f))\n",
    "        \n",
    "        #Simple PLA\n",
    "        \n",
    "        # inital values for each run\n",
    "        iteration = 0 # track the iteration number\n",
    "        weights=[0,0,0] # initial vector weights\n",
    "        errors = [0]*N # inital errors\n",
    "\n",
    "        while sum(errors) < correct_target:\n",
    "\n",
    "            # Calculate the sign of predictions\n",
    "            predictions = gl.SArray(np.sign(np.dot(features, weights)))\n",
    "            \n",
    "            # compare the prediction with output\n",
    "            errors = gl.SArray(predictions == output)       \n",
    "\n",
    "            misclassified_points = list() # track the misclassifed points index\n",
    "            mis_points = list() # track the misclassfied points\n",
    "\n",
    "            for num in xrange(N):\n",
    "                if errors[num] == 0:\n",
    "                    misclassified_points.append(num)\n",
    "                    mis_points.append(features[num,:])\n",
    "\n",
    "            if sum(errors) < correct_target:\n",
    "\n",
    "                # Randomly choose one point from misclassified points (choose index in here) \n",
    "                misclassified_point = random.choice(misclassified_points)\n",
    "\n",
    "                # Recalculate the weights\n",
    "                weights += features[misclassified_point, :] * output[misclassified_point]\n",
    "\n",
    "            else:\n",
    "\n",
    "                iter_success_list.append(iteration) # include the successfully converged iteration number\n",
    "\n",
    "                # predicted function classify the random point\n",
    "                predict_pla = weights[0] + weights[1]*features_3[:,0] + weights[2]*features_3[:,1]              \n",
    "                predict_pla = gl.SArray(np.sign(predict_pla))\n",
    "                \n",
    "                \n",
    "                # include the compared classification result of the random point\n",
    "                prediction_results_pla= sum(np.array(predict_pla) == np.array(target_f))\n",
    "\n",
    "                #print \"PLA Run #\" + str(run) + \": \" + \"successfully converged @ iteration: \" + str(iteration)\n",
    "                #print \"PLA Weights: \" + str(weights)\n",
    "                #print \"PLA Target/Predicted function agree: \" + str(list(predict_pla) == list(target_f))\n",
    "                #print \"======================================================================\"\n",
    "\n",
    "            iteration += 1\n",
    "            \n",
    "            \n",
    "            \n",
    "        # SVM\n",
    "        # Quadratic coefficients\n",
    "        for i in xrange(N):\n",
    "    \n",
    "            f = np.array(([np.dot(output[i]*output[j]*features_2[i,:].transpose(), features_2[j,:]) for j in xrange(N)]))\n",
    "\n",
    "            if i == 0:\n",
    "                qe = np.array(f)\n",
    "            else: \n",
    "                qe = np.vstack((qe, f))\n",
    "        \n",
    "        # cvxopt parameters\n",
    "        P = cvxopt.matrix(qe, tc='d')\n",
    "        q = cvxopt.matrix(np.ones(N) * -1, tc='d')\n",
    "        G = cvxopt.matrix(np.diag(np.ones(N) * -1))\n",
    "        h = cvxopt.matrix(np.zeros(N))\n",
    "        A = cvxopt.matrix(output, (1,N))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "        \n",
    "        # Solve the quadratic program\n",
    "        cvxopt.solvers.options['show_progress'] = False # Turn off the 'show_progress'\n",
    "        \n",
    "        sol = cvxopt.solvers.qp(P,q,G,h,A,b)        \n",
    "                \n",
    "        # Lagrange multipliers\n",
    "        all_a = np.ravel(sol['x'])\n",
    "\n",
    "        # Support vectors have non zero lagrange multipliers   \n",
    "        sv_pos = list()\n",
    "        for i in xrange(len(all_a)):\n",
    "            if all_a[i] > 1e-5:\n",
    "                sv_pos.append(i)\n",
    "  \n",
    "        #print \"%d support vectors out of %d points\" % (len(sv_pos), N)                 \n",
    "        sv_number_list.append(len(sv_pos))        \n",
    "        \n",
    "        \n",
    "        a = all_a[sv_pos] # alpha of the support vectors\n",
    "        sv_features = features_2[sv_pos] # features matrix of support vectors\n",
    "        sv_y = np.array(output)[sv_pos]  # output of support vectors\n",
    "            \n",
    "        # Weights    \n",
    "        weights = np.zeros(2)\n",
    "        for n in xrange(len(a)):\n",
    "            weights += a[n] * sv_y[n] * sv_features[n] \n",
    "        \n",
    "        # Intercept\n",
    "        b_list = list()\n",
    "        \n",
    "        for i in sv_pos:  \n",
    "            b_temp = 1/float(output[i]) - np.dot(weights,features_2[i]) \n",
    "            b_list.append(b_temp)\n",
    "            \n",
    "        b = sum(b_list) / float(len(b_list)) \n",
    "        weights = np.hstack((b, weights)) # insert the intercept to the weights\n",
    "        \n",
    "\n",
    "        # predicted function classify the random point\n",
    "        predict_svm = weights[0] + weights[1]*features_3[:,0] + weights[2]*features_3[:,1]              \n",
    "        predict_svm = gl.SArray(np.sign(predict_svm))\n",
    "\n",
    "\n",
    "        # include the compared classification result of the random point\n",
    "        prediction_results_svm = sum(np.array(predict_svm) == np.array(target_f))\n",
    "\n",
    "        #print \"SVM Weights: \" + str(weights) \n",
    "        #print \"SVM Target/Predicted function agree: \" + str(list(predict_svm) == list(target_f))\n",
    "        #print \"======================================================================\"    \n",
    "        \n",
    "        if prediction_results_svm > prediction_results_pla:\n",
    "            prediction_results.append(1)\n",
    "        else:\n",
    "            prediction_results.append(0)\n",
    "        \n",
    "        run += 1 # update the run number\n",
    "              \n",
    "    print \"Avg count of support vectors: \" +  str(sum(sv_number_list) / float(len(sv_number_list)))\n",
    "\n",
    "    print \"SVM is more efficient on percentage: \" + '{:.4f}'.format(round(sum(prediction_results) / float(len(prediction_results)), 4))\n",
    "\n",
    "    return sum(prediction_results) / float(len(prediction_results))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run # 0 :\n",
      "Run # 1 :\n",
      "Run # 2 :\n",
      "Run # 3 :\n",
      "Run # 4 :\n",
      "Run # 5 :\n",
      "Run # 6 :\n",
      "Run # 7 :\n",
      "Run # 8 :\n",
      "Run # 9 :\n",
      "Run # 10 :\n",
      "Run # 20 :\n",
      "Run # 30 :\n",
      "Run # 40 :\n",
      "Run # 50 :\n",
      "Run # 60 :\n",
      "Run # 70 :\n",
      "Run # 80 :\n",
      "Run # 90 :\n",
      "Run # 100 :\n",
      "Run # 200 :\n",
      "Run # 300 :\n",
      "Run # 400 :\n",
      "Run # 500 :\n",
      "Run # 600 :\n",
      "Run # 700 :\n",
      "Run # 800 :\n",
      "Run # 900 :\n",
      "Avg count of support vectors: 2.876\n",
      "SVM is more efficient on percentage: 0.5560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.556"
      ]
     },
     "execution_count": 1046,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run # 0 :\n",
      "Run # 1 :\n",
      "Run # 2 :\n",
      "Run # 3 :\n",
      "Run # 4 :\n",
      "Run # 5 :\n",
      "Run # 6 :\n",
      "Run # 7 :\n",
      "Run # 8 :\n",
      "Run # 9 :\n",
      "Run # 10 :\n",
      "Run # 20 :\n",
      "Run # 30 :\n",
      "Run # 40 :\n",
      "Run # 50 :\n",
      "Run # 60 :\n",
      "Run # 70 :\n",
      "Run # 80 :\n",
      "Run # 90 :\n",
      "Run # 100 :\n",
      "Run # 200 :\n",
      "Run # 300 :\n",
      "Run # 400 :\n",
      "Run # 500 :\n",
      "Run # 600 :\n",
      "Run # 700 :\n",
      "Run # 800 :\n",
      "Run # 900 :\n",
      "Avg count of support vectors: 3.11\n",
      "SVM is more efficient on percentage: 0.3370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.337"
      ]
     },
     "execution_count": 1047,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
